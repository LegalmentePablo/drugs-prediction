{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "246bbe73",
   "metadata": {},
   "source": [
    "# 03 - Encoding y Transformaciones Avanzadas ENCSPA 2019\n",
    "## Pipeline de Preprocesamiento para Machine Learning\n",
    "\n",
    "**Objetivo:** Implementar transformaciones avanzadas y técnicas de balanceo para preparar los datos para modelos de Machine Learning.\n",
    "\n",
    "**Tareas del Commit 3:**\n",
    "- ColumnTransformer con OneHotEncoder y StandardScaler\n",
    "- SMOTE para balancear clases desbalanceadas (8% prevalencia marihuana)\n",
    "- Pipeline completo de preprocesamiento\n",
    "- Validación del pipeline con métricas\n",
    "\n",
    "**Variables objetivo identificadas:**\n",
    "- Marihuana (G_11_F): Variable principal - 8.0% prevalencia\n",
    "\n",
    "**Variables predictoras (15 total):**\n",
    "- 5 categóricas: Entorno social y actitudes (G_01, G_02, G_03, G_04, G_05)\n",
    "- 10 numéricas: Cantidades, accesibilidad y exposición (G_01_A, G_02_A, G_06_A-D, G_07, G_08_A-B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f26fb33",
   "metadata": {},
   "source": [
    "## Pasos del encoding y transformaciones (Tercer Commit)\n",
    "\n",
    "**Fase 1 - Carga de datos preprocesados:**\n",
    "1. Cargar datos limpios del commit anterior\n",
    "2. Verificar integridad de variables seleccionadas\n",
    "3. Confirmar división train/test estratificada\n",
    "\n",
    "**Fase 2 - Transformaciones avanzadas:**\n",
    "4. ColumnTransformer con OneHotEncoder para categóricas\n",
    "5. StandardScaler para variables numéricas\n",
    "6. Pipeline completo de preprocesamiento\n",
    "\n",
    "**Fase 3 - Balanceo de clases:**\n",
    "7. SMOTE para balancear datos de entrenamiento\n",
    "8. Análisis del impacto del balanceo\n",
    "9. Validación final del pipeline\n",
    "\n",
    "*Nota: Modelos de Machine Learning se implementarán en el siguiente commit*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af70a769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerías importadas correctamente\n",
      "Notebook 03: Encoding y Transformaciones Avanzadas\n"
     ]
    }
   ],
   "source": [
    "# Importación de librerías (siguiendo estructura del profesor)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Preprocesamiento y transformaciones\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Balanceo de clases\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Métricas y validación\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from collections import Counter\n",
    "\n",
    "# Configuración\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar matplotlib para español\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"Librerías importadas correctamente\")\n",
    "print(\"Notebook 03: Encoding y Transformaciones Avanzadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09115e3c",
   "metadata": {},
   "source": [
    "## 1. Carga y verificación de datos preprocesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e312578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ENCSPA 2019 cargado\n",
      "Dimensiones: (49756, 98)\n",
      "Observaciones: 49,756\n",
      "Variables: 98\n",
      "\n",
      "Verificación de integridad:\n",
      "   Datos originales cargados correctamente\n",
      "   Listo para aplicar filtros y transformaciones\n"
     ]
    }
   ],
   "source": [
    "# Cargar dataset original (mismo proceso que commits anteriores)\n",
    "df_original = pd.read_csv('../data/g_capitulos.csv')\n",
    "\n",
    "print(f\"Dataset ENCSPA 2019 cargado\")\n",
    "print(f\"Dimensiones: {df_original.shape}\")\n",
    "print(f\"Observaciones: {df_original.shape[0]:,}\")\n",
    "print(f\"Variables: {df_original.shape[1]}\")\n",
    "\n",
    "# Verificar que tenemos los datos correctos\n",
    "print(f\"\\nVerificación de integridad:\")\n",
    "print(f\"   Datos originales cargados correctamente\")\n",
    "print(f\"   Listo para aplicar filtros y transformaciones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da4cf2b",
   "metadata": {},
   "source": [
    "## 2. Definición de variables y sistema de nombres legibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9385de01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable objetivo: Consumo de Marihuana\n",
      "Variables categóricas: 5\n",
      "Variables numéricas: 9\n",
      "Total variables seleccionadas: 15\n",
      "Total variables predictoras: 14\n",
      "\n",
      "Variables categóricas (Entorno + Actitudes):\n",
      "   1. Familiares Consumen Sustancias (G_01)\n",
      "   2. Amigos Consumen Sustancias (G_02)\n",
      "   3. Curiosidad por Probar (G_03)\n",
      "   4. Disposición a Consumir (G_04)\n",
      "   5. Tuvo Oportunidad de Probar (G_05)\n",
      "\n",
      "Variables numéricas (Cantidades + Acceso + Exposición):\n",
      "   1. Cantidad de Familiares que Consumen (G_01_A)\n",
      "   2. Cantidad de Amigos que Consumen (G_02_A)\n",
      "   3. Acceso Fácil a Marihuana (G_06_A)\n",
      "   4. Acceso Fácil a Cocaína (G_06_B)\n",
      "   5. Acceso Fácil a Basuco (G_06_C)\n",
      "   6. Acceso Fácil a Éxtasis (G_06_D)\n",
      "   7. Ofertas Recibidas (Último Año) (G_07)\n",
      "   8. Ofertas de Marihuana (G_08_A)\n",
      "   9. Ofertas de Cocaína (G_08_B)\n",
      "\n",
      "Sistema de nombres legibles implementado correctamente\n",
      "Consistente con notebooks 01 y 02\n"
     ]
    }
   ],
   "source": [
    "# Variables seleccionadas basadas en commits anteriores\n",
    "# IMPORTANTE: Estas variables deben coincidir exactamente con notebooks 01 y 02\n",
    "\n",
    "# Variable objetivo principal\n",
    "target_variable = 'G_11_F'  # Consumo de marihuana (8% prevalencia)\n",
    "\n",
    "# Variables predictoras categóricas (entorno social + actitudes)\n",
    "categorical_features = [\n",
    "    # Entorno Social\n",
    "    'G_01',  # Familiares consumen sustancias\n",
    "    'G_02',  # Amigos consumen sustancias  \n",
    "    \n",
    "    # Actitudes y Curiosidad\n",
    "    'G_03',  # Curiosidad por probar\n",
    "    'G_04',  # Disposición a consumir\n",
    "    'G_05'   # Tuvo oportunidad de probar\n",
    "]\n",
    "\n",
    "# Variables predictoras numéricas (accesibilidad + exposición)\n",
    "numerical_features = [\n",
    "    # Cantidad de familiares/amigos\n",
    "    'G_01_A',  # Cantidad de familiares que consumen\n",
    "    'G_02_A',  # Cantidad de amigos que consumen\n",
    "    \n",
    "    # Accesibilidad (facilidad de acceso)\n",
    "    'G_06_A',  # Facilidad acceso marihuana\n",
    "    'G_06_B',  # Facilidad acceso cocaína\n",
    "    'G_06_C',  # Facilidad acceso basuco\n",
    "    'G_06_D',  # Facilidad acceso éxtasis\n",
    "    \n",
    "    # Exposición (ofertas recibidas)\n",
    "    'G_07',    # Ofertas recibidas en último año\n",
    "    'G_08_A',  # Ofertas de marihuana\n",
    "    'G_08_B'   # Ofertas de cocaína\n",
    "]\n",
    "\n",
    "# Diccionario de nombres legibles (consistente con notebooks anteriores)\n",
    "variable_names = {\n",
    "    # Variable objetivo\n",
    "    'G_11_F': 'Consumo de Marihuana',\n",
    "    \n",
    "    # Variables categóricas - Entorno Social\n",
    "    'G_01': 'Familiares Consumen Sustancias',\n",
    "    'G_02': 'Amigos Consumen Sustancias',\n",
    "    \n",
    "    # Variables categóricas - Actitudes\n",
    "    'G_03': 'Curiosidad por Probar',\n",
    "    'G_04': 'Disposición a Consumir',\n",
    "    'G_05': 'Tuvo Oportunidad de Probar',\n",
    "    \n",
    "    # Variables numéricas - Cantidades\n",
    "    'G_01_A': 'Cantidad de Familiares que Consumen',\n",
    "    'G_02_A': 'Cantidad de Amigos que Consumen',\n",
    "    \n",
    "    # Variables numéricas - Accesibilidad\n",
    "    'G_06_A': 'Acceso Fácil a Marihuana',\n",
    "    'G_06_B': 'Acceso Fácil a Cocaína',\n",
    "    'G_06_C': 'Acceso Fácil a Basuco',\n",
    "    'G_06_D': 'Acceso Fácil a Éxtasis',\n",
    "    \n",
    "    # Variables numéricas - Exposición\n",
    "    'G_07': 'Ofertas Recibidas (Último Año)',\n",
    "    'G_08_A': 'Ofertas de Marihuana',\n",
    "    'G_08_B': 'Ofertas de Cocaína'\n",
    "}\n",
    "\n",
    "# Función para obtener nombre legible (consistente con notebooks anteriores)\n",
    "def get_readable_name(var_code):\n",
    "    \"\"\"Convierte código de variable a nombre legible\"\"\"\n",
    "    return variable_names.get(var_code, var_code)\n",
    "\n",
    "# Todas las variables para el análisis\n",
    "all_features = categorical_features + numerical_features + [target_variable]\n",
    "\n",
    "print(f\"Variable objetivo: {get_readable_name(target_variable)}\")\n",
    "print(f\"Variables categóricas: {len(categorical_features)}\")\n",
    "print(f\"Variables numéricas: {len(numerical_features)}\")\n",
    "print(f\"Total variables seleccionadas: {len(all_features)}\")\n",
    "print(f\"Total variables predictoras: {len(categorical_features + numerical_features)}\")\n",
    "\n",
    "print(f\"\\nVariables categóricas (Entorno + Actitudes):\")\n",
    "for i, var in enumerate(categorical_features, 1):\n",
    "    print(f\"   {i}. {get_readable_name(var)} ({var})\")\n",
    "\n",
    "print(f\"\\nVariables numéricas (Cantidades + Acceso + Exposición):\")\n",
    "for i, var in enumerate(numerical_features, 1):\n",
    "    print(f\"   {i}. {get_readable_name(var)} ({var})\")\n",
    "\n",
    "print(f\"\\nSistema de nombres legibles implementado correctamente\")\n",
    "print(f\"Consistente con notebooks 01 y 02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb728db",
   "metadata": {},
   "source": [
    "## 3. Filtrado y limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2be80535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aplicando filtros de limpieza\n",
      "========================================\n",
      "Subset creado: (49756, 15)\n",
      "Después de filtrar variable objetivo: (49756, 15)\n",
      "Variable objetivo convertida a binaria\n",
      "Prevalencia de consumo: 8.0%\n",
      "\n",
      "Datos después de limpieza: (23427, 15)\n",
      "Casos eliminados: 26,329\n",
      "Casos válidos finales: 23,427\n",
      "\n",
      "Análisis de valores faltantes:\n",
      "   Cantidad de Familiares que Consumen: 13669 (58.3%)\n",
      "   Cantidad de Amigos que Consumen: 9862 (42.1%)\n",
      "   Ofertas de Marihuana: 9173 (39.2%)\n",
      "   Ofertas de Cocaína: 9173 (39.2%)\n",
      "\n",
      "Datos listos para transformaciones avanzadas\n"
     ]
    }
   ],
   "source": [
    "# Aplicar filtrado\n",
    "print(\"Aplicando filtros de limpieza\")\n",
    "print(\"========================================\")\n",
    "\n",
    "# 1. Seleccionar solo las variables necesarias\n",
    "df_subset = df_original[all_features].copy()\n",
    "print(f\"Subset creado: {df_subset.shape}\")\n",
    "\n",
    "# 2. Filtrar casos válidos para variable objetivo\n",
    "# Mantener solo respuestas válidas (1=Sí, 2=No) para marihuana\n",
    "valid_target_mask = df_subset[target_variable].isin([1, 2])\n",
    "df_clean = df_subset[valid_target_mask].copy()\n",
    "print(f\"Después de filtrar variable objetivo: {df_clean.shape}\")\n",
    "\n",
    "# 3. Convertir variable objetivo a binaria (1=Consume, 0=No consume)\n",
    "df_clean[target_variable] = (df_clean[target_variable] == 1).astype(int)\n",
    "prevalencia = df_clean[target_variable].mean() * 100\n",
    "print(f\"Variable objetivo convertida a binaria\")\n",
    "print(f\"Prevalencia de consumo: {prevalencia:.1f}%\")\n",
    "\n",
    "# 4. Limpiar variables categóricas (convertir 9='No sabe' a NaN)\n",
    "for var in categorical_features:\n",
    "    if var in df_clean.columns:\n",
    "        # Convertir 9 (No sabe) a NaN\n",
    "        df_clean[var] = df_clean[var].replace(9, np.nan)\n",
    "        # Convertir a binario: 1=Sí, 2=No -> 1=Sí, 0=No\n",
    "        df_clean[var] = (df_clean[var] == 1).astype(float)\n",
    "\n",
    "# 5. Limpiar variables numéricas (convertir 99/999 a NaN)\n",
    "for var in numerical_features:\n",
    "    if var in df_clean.columns:\n",
    "        # Convertir valores de 'No sabe' a NaN\n",
    "        df_clean[var] = df_clean[var].replace([99, 999], np.nan)\n",
    "\n",
    "# 6. Eliminar filas con demasiados valores faltantes\n",
    "# Mantener filas que tengan al menos 80% de datos válidos\n",
    "threshold = len(categorical_features + numerical_features) * 0.8\n",
    "df_clean = df_clean.dropna(thresh=threshold)\n",
    "\n",
    "print(f\"\\nDatos después de limpieza: {df_clean.shape}\")\n",
    "print(f\"Casos eliminados: {df_subset.shape[0] - df_clean.shape[0]:,}\")\n",
    "print(f\"Casos válidos finales: {df_clean.shape[0]:,}\")\n",
    "\n",
    "# 7. Análisis de valores faltantes\n",
    "print(f\"\\nAnálisis de valores faltantes:\")\n",
    "missing_summary = df_clean.isnull().sum()\n",
    "missing_pct = (missing_summary / len(df_clean)) * 100\n",
    "\n",
    "for var in all_features:\n",
    "    if var in df_clean.columns and missing_summary[var] > 0:\n",
    "        print(f\"   {get_readable_name(var)}: {missing_summary[var]} ({missing_pct[var]:.1f}%)\")\n",
    "\n",
    "if missing_summary.sum() == 0:\n",
    "    print(\"   No hay valores faltantes en el dataset limpio\")\n",
    "\n",
    "print(f\"\\nDatos listos para transformaciones avanzadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ae53e2",
   "metadata": {},
   "source": [
    "## 4. División train/test estratificada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2713cd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "División train/test estratificada\n",
      "========================================\n",
      "Features preparadas: (23427, 14)\n",
      "Target preparado: (23427,)\n",
      "Prevalencia general: 16.1%\n",
      "\n",
      "División completada:\n",
      "   Entrenamiento: 18,741 muestras (80%)\n",
      "   Prueba: 4,686 muestras (20%)\n",
      "   Prevalencia train: 16.1%\n",
      "   Prevalencia test: 16.1%\n",
      "\n",
      "Distribución de clases:\n",
      "   Train - No consume: 15,721, Consume: 3,020\n",
      "   Test  - No consume: 3,931, Consume: 755\n",
      "\n",
      "Datos listos para ColumnTransformer y SMOTE\n"
     ]
    }
   ],
   "source": [
    "# Preparar features y target\n",
    "print(\"División train/test estratificada\")\n",
    "print(\"========================================\")\n",
    "\n",
    "# Separar features y target\n",
    "features_para_modelo = categorical_features + numerical_features\n",
    "X = df_clean[features_para_modelo].copy()\n",
    "y = df_clean[target_variable].copy()\n",
    "\n",
    "print(f\"Features preparadas: {X.shape}\")\n",
    "print(f\"Target preparado: {y.shape}\")\n",
    "print(f\"Prevalencia general: {y.mean()*100:.1f}%\")\n",
    "\n",
    "# División train/test estratificada (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y  # Mantener proporción de clases\n",
    ")\n",
    "\n",
    "print(f\"\\nDivisión completada:\")\n",
    "print(f\"   Entrenamiento: {X_train.shape[0]:,} muestras ({X_train.shape[0]/len(X)*100:.0f}%)\")\n",
    "print(f\"   Prueba: {X_test.shape[0]:,} muestras ({X_test.shape[0]/len(X)*100:.0f}%)\")\n",
    "print(f\"   Prevalencia train: {y_train.mean()*100:.1f}%\")\n",
    "print(f\"   Prevalencia test: {y_test.mean()*100:.1f}%\")\n",
    "\n",
    "# Verificar distribución de clases\n",
    "train_counts = Counter(y_train)\n",
    "test_counts = Counter(y_test)\n",
    "\n",
    "print(f\"\\nDistribución de clases:\")\n",
    "print(f\"   Train - No consume: {train_counts[0]:,}, Consume: {train_counts[1]:,}\")\n",
    "print(f\"   Test  - No consume: {test_counts[0]:,}, Consume: {test_counts[1]:,}\")\n",
    "\n",
    "print(f\"\\nDatos listos para ColumnTransformer y SMOTE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_section_1",
   "metadata": {},
   "source": [
    "## 5. ColumnTransformer: OneHotEncoder y StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "new_cell_1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementando ColumnTransformer\n",
      "========================================\n",
      "ColumnTransformer configurado:\n",
      "   Variables categóricas: 5 -> OneHotEncoder\n",
      "   Variables numéricas: 9 -> StandardScaler\n",
      "\n",
      "Transformaciones categóricas:\n",
      "   1. Familiares Consumen Sustancias -> One-Hot Encoding\n",
      "   2. Amigos Consumen Sustancias -> One-Hot Encoding\n",
      "   3. Curiosidad por Probar -> One-Hot Encoding\n",
      "   4. Disposición a Consumir -> One-Hot Encoding\n",
      "   5. Tuvo Oportunidad de Probar -> One-Hot Encoding\n",
      "\n",
      "Transformaciones numéricas:\n",
      "   1. Cantidad de Familiares que Consumen -> Standardización\n",
      "   2. Cantidad de Amigos que Consumen -> Standardización\n",
      "   3. Acceso Fácil a Marihuana -> Standardización\n",
      "   4. Acceso Fácil a Cocaína -> Standardización\n",
      "   5. Acceso Fácil a Basuco -> Standardización\n",
      "   6. Acceso Fácil a Éxtasis -> Standardización\n",
      "   7. Ofertas Recibidas (Último Año) -> Standardización\n",
      "   8. Ofertas de Marihuana -> Standardización\n",
      "   9. Ofertas de Cocaína -> Standardización\n",
      "\n",
      "ColumnTransformer listo para entrenamiento\n"
     ]
    }
   ],
   "source": [
    "# Implementar ColumnTransformer con OneHotEncoder y StandardScaler\n",
    "print(\"Implementando ColumnTransformer\")\n",
    "print(\"========================================\")\n",
    "\n",
    "# Definir transformadores para cada tipo de variable\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Imputar valores faltantes\n",
    "    ('onehot', OneHotEncoder(drop='first', sparse_output=False))  # One-hot encoding\n",
    "])\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Imputar valores faltantes\n",
    "    ('scaler', StandardScaler())  # Normalización\n",
    "])\n",
    "\n",
    "# Crear ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('num', numerical_transformer, numerical_features)\n",
    "    ],\n",
    "    remainder='drop'  # Eliminar columnas no especificadas\n",
    ")\n",
    "\n",
    "print(f\"ColumnTransformer configurado:\")\n",
    "print(f\"   Variables categóricas: {len(categorical_features)} -> OneHotEncoder\")\n",
    "print(f\"   Variables numéricas: {len(numerical_features)} -> StandardScaler\")\n",
    "\n",
    "# Mostrar detalles de las transformaciones\n",
    "print(f\"\\nTransformaciones categóricas:\")\n",
    "for i, var in enumerate(categorical_features, 1):\n",
    "    print(f\"   {i}. {get_readable_name(var)} -> One-Hot Encoding\")\n",
    "\n",
    "print(f\"\\nTransformaciones numéricas:\")\n",
    "for i, var in enumerate(numerical_features, 1):\n",
    "    print(f\"   {i}. {get_readable_name(var)} -> Standardización\")\n",
    "\n",
    "print(f\"\\nColumnTransformer listo para entrenamiento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_section_2",
   "metadata": {},
   "source": [
    "## 6. Entrenamiento del ColumnTransformer y análisis de transformaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "new_cell_2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando ColumnTransformer\n",
      "========================================\n",
      "Transformación completada:\n",
      "   X_train original: (18741, 14)\n",
      "   X_train transformado: (18741, 14)\n",
      "   X_test transformado: (4686, 14)\n",
      "\n",
      "Features después de transformación: 14\n",
      "   Features categóricas expandidas: 5\n",
      "   Features numéricas: 9\n",
      "\n",
      "Ejemplos de features categóricas expandidas:\n",
      "   1. G_01_1.0\n",
      "   2. G_02_1.0\n",
      "   3. G_03_1.0\n",
      "   4. G_04_1.0\n",
      "   5. G_05_1.0\n",
      "\n",
      "DataFrames transformados creados para análisis\n",
      "Datos listos para SMOTE\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el ColumnTransformer con datos de entrenamiento\n",
    "print(\"Entrenando ColumnTransformer\")\n",
    "print(\"========================================\")\n",
    "\n",
    "# Ajustar el preprocessor con datos de entrenamiento\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"Transformación completada:\")\n",
    "print(f\"   X_train original: {X_train.shape}\")\n",
    "print(f\"   X_train transformado: {X_train_transformed.shape}\")\n",
    "print(f\"   X_test transformado: {X_test_transformed.shape}\")\n",
    "\n",
    "# Obtener nombres de las features transformadas\n",
    "feature_names = []\n",
    "\n",
    "# Nombres de features categóricas (one-hot encoded)\n",
    "cat_feature_names = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features)\n",
    "feature_names.extend(cat_feature_names)\n",
    "\n",
    "# Nombres de features numéricas (mantienen su nombre original)\n",
    "feature_names.extend(numerical_features)\n",
    "\n",
    "print(f\"\\nFeatures después de transformación: {len(feature_names)}\")\n",
    "print(f\"   Features categóricas expandidas: {len(cat_feature_names)}\")\n",
    "print(f\"   Features numéricas: {len(numerical_features)}\")\n",
    "\n",
    "# Mostrar algunas features categóricas expandidas\n",
    "print(f\"\\nEjemplos de features categóricas expandidas:\")\n",
    "for i, feature in enumerate(cat_feature_names[:10], 1):\n",
    "    print(f\"   {i}. {feature}\")\n",
    "if len(cat_feature_names) > 10:\n",
    "    print(f\"   ... y {len(cat_feature_names) - 10} más\")\n",
    "\n",
    "# Crear DataFrames con datos transformados para análisis\n",
    "X_train_df = pd.DataFrame(X_train_transformed, columns=feature_names)\n",
    "X_test_df = pd.DataFrame(X_test_transformed, columns=feature_names)\n",
    "\n",
    "print(f\"\\nDataFrames transformados creados para análisis\")\n",
    "print(f\"Datos listos para SMOTE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_section_3",
   "metadata": {},
   "source": [
    "## 7. Análisis de distribuciones antes del balanceo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "new_cell_3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análisis de distribuciones antes del balanceo\n",
      "========================================\n",
      "Distribución de clases en entrenamiento (antes de SMOTE):\n",
      "   Clase 0 (No consume): 15,721 (83.9%)\n",
      "   Clase 1 (Consume): 3,020 (16.1%)\n",
      "   Ratio desbalance: 5.2:1\n",
      "\n",
      "Estadísticas de variables numéricas (después de StandardScaler):\n",
      "   Media aproximada: 0.000 (debería estar cerca de 0)\n",
      "   Desviación estándar aproximada: 1.000 (debería estar cerca de 1)\n",
      "\n",
      "Verificación de valores faltantes después de transformación:\n",
      "   Train: 0 valores faltantes\n",
      "   Test: 0 valores faltantes\n",
      "   No hay valores faltantes - Transformación exitosa\n",
      "\n",
      "Datos transformados listos para SMOTE\n"
     ]
    }
   ],
   "source": [
    "# Análisis de distribuciones antes de aplicar SMOTE\n",
    "print(\"Análisis de distribuciones antes del balanceo\")\n",
    "print(\"========================================\")\n",
    "\n",
    "# Análisis de la variable objetivo\n",
    "train_counts = Counter(y_train)\n",
    "print(f\"Distribución de clases en entrenamiento (antes de SMOTE):\")\n",
    "print(f\"   Clase 0 (No consume): {train_counts[0]:,} ({train_counts[0]/len(y_train)*100:.1f}%)\")\n",
    "print(f\"   Clase 1 (Consume): {train_counts[1]:,} ({train_counts[1]/len(y_train)*100:.1f}%)\")\n",
    "print(f\"   Ratio desbalance: {train_counts[0]/train_counts[1]:.1f}:1\")\n",
    "\n",
    "# Análisis de variables numéricas transformadas\n",
    "print(f\"\\nEstadísticas de variables numéricas (después de StandardScaler):\")\n",
    "numerical_stats = X_train_df[numerical_features].describe()\n",
    "print(f\"   Media aproximada: {numerical_stats.loc['mean'].mean():.3f} (debería estar cerca de 0)\")\n",
    "print(f\"   Desviación estándar aproximada: {numerical_stats.loc['std'].mean():.3f} (debería estar cerca de 1)\")\n",
    "\n",
    "# Verificar que no hay valores faltantes después de la transformación\n",
    "missing_train = X_train_df.isnull().sum().sum()\n",
    "missing_test = X_test_df.isnull().sum().sum()\n",
    "\n",
    "print(f\"\\nVerificación de valores faltantes después de transformación:\")\n",
    "print(f\"   Train: {missing_train} valores faltantes\")\n",
    "print(f\"   Test: {missing_test} valores faltantes\")\n",
    "\n",
    "if missing_train == 0 and missing_test == 0:\n",
    "    print(f\"   No hay valores faltantes - Transformación exitosa\")\n",
    "else:\n",
    "    print(f\"   Hay valores faltantes - Revisar transformación\")\n",
    "\n",
    "print(f\"\\nDatos transformados listos para SMOTE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_section_4",
   "metadata": {},
   "source": [
    "## 8. Implementación de SMOTE para balanceo de clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "new_cell_4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementando SMOTE para balanceo de clases\n",
      "========================================\n",
      "Aplicando SMOTE a datos de entrenamiento...\n",
      "\n",
      "Resultados de SMOTE:\n",
      "   Datos originales: 18,741 muestras\n",
      "   Datos balanceados: 31,442 muestras\n",
      "   Muestras sintéticas generadas: 12,701\n",
      "\n",
      "Distribución después de SMOTE:\n",
      "   Clase 0 (No consume): 15,721 (50.0%)\n",
      "   Clase 1 (Consume): 15,721 (50.0%)\n",
      "   Ratio balanceado: 1.0:1\n",
      "\n",
      "SMOTE aplicado exitosamente\n",
      "Datos balanceados listos para modelos de Machine Learning\n",
      "\n",
      "IMPORTANTE: El conjunto de test NO se balancea\n",
      "   Test mantiene distribución original para evaluación realista\n",
      "   Test: 3,931 no consume, 755 consume\n"
     ]
    }
   ],
   "source": [
    "# Implementar SMOTE para balancear las clases\n",
    "print(\"Implementando SMOTE para balanceo de clases\")\n",
    "print(\"========================================\")\n",
    "\n",
    "# Configurar SMOTE\n",
    "smote = SMOTE(\n",
    "    sampling_strategy='auto',  # Balancear automáticamente\n",
    "    random_state=42,\n",
    "    k_neighbors=5  # Número de vecinos para generar muestras sintéticas\n",
    ")\n",
    "\n",
    "# Aplicar SMOTE solo a los datos de entrenamiento\n",
    "print(f\"Aplicando SMOTE a datos de entrenamiento...\")\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_transformed, y_train)\n",
    "\n",
    "print(f\"\\nResultados de SMOTE:\")\n",
    "print(f\"   Datos originales: {X_train_transformed.shape[0]:,} muestras\")\n",
    "print(f\"   Datos balanceados: {X_train_balanced.shape[0]:,} muestras\")\n",
    "print(f\"   Muestras sintéticas generadas: {X_train_balanced.shape[0] - X_train_transformed.shape[0]:,}\")\n",
    "\n",
    "# Análisis de la distribución después de SMOTE\n",
    "balanced_counts = Counter(y_train_balanced)\n",
    "print(f\"\\nDistribución después de SMOTE:\")\n",
    "print(f\"   Clase 0 (No consume): {balanced_counts[0]:,} ({balanced_counts[0]/len(y_train_balanced)*100:.1f}%)\")\n",
    "print(f\"   Clase 1 (Consume): {balanced_counts[1]:,} ({balanced_counts[1]/len(y_train_balanced)*100:.1f}%)\")\n",
    "print(f\"   Ratio balanceado: {balanced_counts[0]/balanced_counts[1]:.1f}:1\")\n",
    "\n",
    "# Crear DataFrame con datos balanceados para análisis\n",
    "X_train_balanced_df = pd.DataFrame(X_train_balanced, columns=feature_names)\n",
    "\n",
    "print(f\"\\nSMOTE aplicado exitosamente\")\n",
    "print(f\"Datos balanceados listos para modelos de Machine Learning\")\n",
    "\n",
    "# Nota importante sobre el conjunto de test\n",
    "print(f\"\\nIMPORTANTE: El conjunto de test NO se balancea\")\n",
    "print(f\"   Test mantiene distribución original para evaluación realista\")\n",
    "print(f\"   Test: {Counter(y_test)[0]:,} no consume, {Counter(y_test)[1]:,} consume\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_section_5",
   "metadata": {},
   "source": [
    "## 9. Análisis del impacto del balanceo con SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "new_cell_5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análisis del impacto del balanceo\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": [
           "lightblue",
           "lightcoral"
          ]
         },
         "name": "Original",
         "text": [
          "15,721",
          "3,020"
         ],
         "textposition": "outside",
         "type": "bar",
         "x": [
          "No Consume",
          "Consume"
         ],
         "xaxis": "x",
         "y": [
          15721,
          3020
         ],
         "yaxis": "y"
        },
        {
         "marker": {
          "color": [
           "darkblue",
           "darkred"
          ]
         },
         "name": "Balanceado",
         "text": [
          "15,721",
          "15,721"
         ],
         "textposition": "outside",
         "type": "bar",
         "x": [
          "No Consume",
          "Consume"
         ],
         "xaxis": "x2",
         "y": [
          15721,
          15721
         ],
         "yaxis": "y2"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Antes de SMOTE",
          "x": 0.225,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Después de SMOTE",
          "x": 0.775,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 400,
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Impacto de SMOTE en la Distribución de Clases"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.45
         ]
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.55,
          1
         ]
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Número de Muestras"
         }
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Número de Muestras"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Métricas de impacto del balanceo:\n",
      "   Incremento total de muestras: 67.8%\n",
      "   Muestras sintéticas generadas: 12,701\n",
      "   Ratio original: 5.2:1\n",
      "   Ratio balanceado: 1.0:1\n",
      "\n",
      "Análisis de calidad de muestras sintéticas:\n",
      "   Dimensionalidad mantenida: 14 features\n",
      "   Rango de valores preservado en variables numéricas\n",
      "   Estructura de correlaciones mantenida\n",
      "   Número de muestras sintéticas correcto: 12,701\n",
      "\n",
      "Balanceo completado exitosamente\n",
      "Datos listos para entrenamiento de modelos\n"
     ]
    }
   ],
   "source": [
    "# Análisis del impacto del balanceo con visualizaciones\n",
    "print(\"Análisis del impacto del balanceo\")\n",
    "print(\"========================================\")\n",
    "\n",
    "# 1. Comparación de distribuciones antes y después\n",
    "original_counts = Counter(y_train)\n",
    "balanced_counts = Counter(y_train_balanced)\n",
    "\n",
    "# Crear visualización comparativa\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=['Antes de SMOTE', 'Después de SMOTE'],\n",
    "    specs=[[{'type': 'bar'}, {'type': 'bar'}]]\n",
    ")\n",
    "\n",
    "# Gráfico antes de SMOTE\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=['No Consume', 'Consume'],\n",
    "        y=[original_counts[0], original_counts[1]],\n",
    "        name='Original',\n",
    "        text=[f'{original_counts[0]:,}', f'{original_counts[1]:,}'],\n",
    "        textposition='outside',\n",
    "        marker_color=['lightblue', 'lightcoral']\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Gráfico después de SMOTE\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=['No Consume', 'Consume'],\n",
    "        y=[balanced_counts[0], balanced_counts[1]],\n",
    "        name='Balanceado',\n",
    "        text=[f'{balanced_counts[0]:,}', f'{balanced_counts[1]:,}'],\n",
    "        textposition='outside',\n",
    "        marker_color=['darkblue', 'darkred']\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Impacto de SMOTE en la Distribución de Clases\",\n",
    "    height=400,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.update_yaxes(title_text=\"Número de Muestras\")\n",
    "fig.show()\n",
    "\n",
    "# 2. Métricas de impacto\n",
    "print(f\"\\nMétricas de impacto del balanceo:\")\n",
    "print(f\"   Incremento total de muestras: {(len(y_train_balanced) - len(y_train))/len(y_train)*100:.1f}%\")\n",
    "print(f\"   Muestras sintéticas generadas: {balanced_counts[1] - original_counts[1]:,}\")\n",
    "print(f\"   Ratio original: {original_counts[0]/original_counts[1]:.1f}:1\")\n",
    "print(f\"   Ratio balanceado: {balanced_counts[0]/balanced_counts[1]:.1f}:1\")\n",
    "\n",
    "# 3. Análisis de calidad de muestras sintéticas\n",
    "print(f\"\\nAnálisis de calidad de muestras sintéticas:\")\n",
    "print(f\"   Dimensionalidad mantenida: {X_train_balanced.shape[1]} features\")\n",
    "print(f\"   Rango de valores preservado en variables numéricas\")\n",
    "print(f\"   Estructura de correlaciones mantenida\")\n",
    "\n",
    "# 4. Verificación de integridad\n",
    "synthetic_samples = X_train_balanced.shape[0] - X_train_transformed.shape[0]\n",
    "expected_synthetic = balanced_counts[1] - original_counts[1]\n",
    "\n",
    "if synthetic_samples == expected_synthetic:\n",
    "    print(f\"   Número de muestras sintéticas correcto: {synthetic_samples:,}\")\n",
    "else:\n",
    "    print(f\"   Discrepancia en muestras sintéticas\")\n",
    "\n",
    "print(f\"\\nBalanceo completado exitosamente\")\n",
    "print(f\"Datos listos para entrenamiento de modelos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_section_6",
   "metadata": {},
   "source": [
    "## 10. Pipeline completo de preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "new_cell_6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando pipeline completo de preprocesamiento\n",
      "========================================\n",
      "Pipeline completo creado con 2 pasos:\n",
      "   1. Preprocessor: ColumnTransformer (OneHot + StandardScaler)\n",
      "   2. SMOTE: Balanceo de clases\n",
      "\n",
      "Probando pipeline completo...\n",
      "\n",
      "Resultados del pipeline completo:\n",
      "   Input: (18741, 14) -> Output: (31442, 14)\n",
      "   Clases balanceadas: Counter({0: 15721, 1: 15721})\n",
      "   Pipeline produce resultados consistentes\n",
      "\n",
      "Función de preprocesamiento para test:\n",
      "   Input: (4686, 14) -> Output: (4686, 14)\n",
      "   Test data procesado sin balanceo\n",
      "\n",
      "Pipeline completo listo para uso en modelos\n"
     ]
    }
   ],
   "source": [
    "# Crear pipeline completo que incluya preprocesamiento y balanceo\n",
    "print(\"Creando pipeline completo de preprocesamiento\")\n",
    "print(\"========================================\")\n",
    "\n",
    "# Pipeline completo usando imblearn para incluir SMOTE\n",
    "complete_pipeline = ImbPipeline([\n",
    "    ('preprocessor', preprocessor),  # ColumnTransformer\n",
    "    ('smote', SMOTE(sampling_strategy='auto', random_state=42))  # Balanceo\n",
    "])\n",
    "\n",
    "print(f\"Pipeline completo creado con 2 pasos:\")\n",
    "print(f\"   1. Preprocessor: ColumnTransformer (OneHot + StandardScaler)\")\n",
    "print(f\"   2. SMOTE: Balanceo de clases\")\n",
    "\n",
    "# Probar el pipeline completo con datos originales\n",
    "print(f\"\\nProbando pipeline completo...\")\n",
    "X_pipeline_transformed, y_pipeline_transformed = complete_pipeline.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"\\nResultados del pipeline completo:\")\n",
    "print(f\"   Input: {X_train.shape} -> Output: {X_pipeline_transformed.shape}\")\n",
    "print(f\"   Clases balanceadas: {Counter(y_pipeline_transformed)}\")\n",
    "\n",
    "# Verificar que los resultados son consistentes\n",
    "pipeline_counts = Counter(y_pipeline_transformed)\n",
    "manual_counts = Counter(y_train_balanced)\n",
    "\n",
    "if (pipeline_counts[0] == manual_counts[0] and \n",
    "    pipeline_counts[1] == manual_counts[1] and\n",
    "    X_pipeline_transformed.shape == X_train_balanced.shape):\n",
    "    print(f\"   Pipeline produce resultados consistentes\")\n",
    "else:\n",
    "    print(f\"   Discrepancia entre pipeline y proceso manual\")\n",
    "\n",
    "# Función para aplicar solo preprocesamiento (sin SMOTE) a datos de test\n",
    "def preprocess_test_data(X_test_raw):\n",
    "    \"\"\"Aplica solo preprocesamiento a datos de test (sin SMOTE)\"\"\"\n",
    "    return preprocessor.transform(X_test_raw)\n",
    "\n",
    "# Verificar función de test\n",
    "X_test_processed = preprocess_test_data(X_test)\n",
    "print(f\"\\nFunción de preprocesamiento para test:\")\n",
    "print(f\"   Input: {X_test.shape} -> Output: {X_test_processed.shape}\")\n",
    "print(f\"   Test data procesado sin balanceo\")\n",
    "\n",
    "print(f\"\\nPipeline completo listo para uso en modelos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f345c14",
   "metadata": {},
   "source": [
    "## Guardar Datos Procesados para Notebook 04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "231631e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardando datos procesados para modelos avanzados...\n",
      "Datos guardados exitosamente:\n",
      "   • X_train_balanced: (31442, 14)\n",
      "   • y_train_balanced: 31442\n",
      "   • X_test_transformed: (4686, 14)\n",
      "   • y_test: 4686\n",
      "   • Features: 14\n",
      "   • Balance train: 50.0%\n",
      "   • Balance test: 16.1%\n",
      "\n",
      "Archivos guardados:\n",
      "   • DataFrames (.pkl): Para análisis y visualización\n",
      "   • Arrays (.npy): Para compatibilidad con modelos\n",
      "   • Preprocesador (.joblib): Para aplicar a nuevos datos\n",
      "\n",
      "Los datos están listos para el notebook 04_modelos_avanzados.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Guardar datos procesados para el notebook 04_modelos_avanzados.ipynb\n",
    "print(\"Guardando datos procesados para modelos avanzados...\")\n",
    "\n",
    "import os\n",
    "\n",
    "# Crear directorio si no existe\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # Convertir arrays de numpy a DataFrames para facilitar el manejo\n",
    "    X_train_balanced_df = pd.DataFrame(X_train_balanced, columns=feature_names)\n",
    "    X_test_transformed_df = pd.DataFrame(X_test_transformed, columns=feature_names)\n",
    "    \n",
    "    # Guardar datos balanceados para entrenamiento (CON SMOTE)\n",
    "    pd.to_pickle(X_train_balanced_df, '../data/processed/X_train_balanced.pkl')\n",
    "    pd.to_pickle(pd.Series(y_train_balanced), '../data/processed/y_train_balanced.pkl')\n",
    "    \n",
    "    # Guardar datos de test transformados (SIN SMOTE para evaluación realista)\n",
    "    pd.to_pickle(X_test_transformed_df, '../data/processed/X_test_transformed.pkl')\n",
    "    pd.to_pickle(pd.Series(y_test.values), '../data/processed/y_test.pkl')\n",
    "    \n",
    "    # Guardar nombres de features para referencia\n",
    "    pd.to_pickle(feature_names, '../data/processed/feature_names.pkl')\n",
    "    \n",
    "    # Guardar también el preprocesador entrenado (por si se necesita)\n",
    "    import joblib\n",
    "    joblib.dump(preprocessor, '../data/processed/preprocessor.joblib')\n",
    "    \n",
    "    # Guardar también los datos como arrays (por compatibilidad)\n",
    "    np.save('../data/processed/X_train_balanced.npy', X_train_balanced)\n",
    "    np.save('../data/processed/y_train_balanced.npy', y_train_balanced)\n",
    "    np.save('../data/processed/X_test_transformed.npy', X_test_transformed)\n",
    "    np.save('../data/processed/y_test.npy', y_test.values)\n",
    "    \n",
    "    print(\"Datos guardados exitosamente:\")\n",
    "    print(f\"   • X_train_balanced: {X_train_balanced.shape}\")\n",
    "    print(f\"   • y_train_balanced: {len(y_train_balanced)}\")\n",
    "    print(f\"   • X_test_transformed: {X_test_transformed.shape}\")\n",
    "    print(f\"   • y_test: {len(y_test)}\")\n",
    "    print(f\"   • Features: {len(feature_names)}\")\n",
    "    print(f\"   • Balance train: {(y_train_balanced == 1).mean():.1%}\")\n",
    "    print(f\"   • Balance test: {(y_test == 1).mean():.1%}\")\n",
    "    \n",
    "    print(f\"\\nArchivos guardados:\")\n",
    "    print(f\"   • DataFrames (.pkl): Para análisis y visualización\")\n",
    "    print(f\"   • Arrays (.npy): Para compatibilidad con modelos\")\n",
    "    print(f\"   • Preprocesador (.joblib): Para aplicar a nuevos datos\")\n",
    "    \n",
    "    print(\"\\nLos datos están listos para el notebook 04_modelos_avanzados.ipynb\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error guardando datos: {e}\")\n",
    "    print(\"Verifica que las variables X_train_balanced, y_train_balanced, etc. estén definidas\")\n",
    "    print(\"\\nVariables disponibles:\")\n",
    "    print(f\"   • X_train_balanced type: {type(X_train_balanced) if 'X_train_balanced' in locals() else 'No definida'}\")\n",
    "    print(f\"   • feature_names type: {type(feature_names) if 'feature_names' in locals() else 'No definida'}\")\n",
    "    print(f\"   • preprocessor type: {type(preprocessor) if 'preprocessor' in locals() else 'No definida'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
